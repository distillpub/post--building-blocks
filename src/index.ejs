<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="template.v2.js"></script>

  <style>
    body {
      --gray-bg: hsl(0, 0%, 97%);
      --gray-border: rgba(0, 0, 0, 0.1);
      --border-radius: 7px;
    }

    d-title {
      overflow-y: hidden;
      padding-bottom: 0;
    }

    d-title h1 {
      grid-column: page;
      text-align: center;
      margin-bottom: 25px;
    }

    #Teaser {
      display: flex;
      justify-content: center;
      margin-bottom: 0;
      background: var(--gray-bg);
      border-top: 1px solid var(--gray-border);
      padding: 60px 0;
    }
    
    figure.full-width {
      grid-column: screen;
      padding-left: 90px;
    }

    figure.act-grids {
      overflow: hidden;
    }

    #AttributionSpatial {
      background: var(--gray-bg);
      padding: 20px 0;
      border-top: 1px solid var(--gray-border);
      border-bottom: 1px solid var(--gray-border);
    }

    #AttributionChannel {
      background: var(--gray-bg);
      padding-top: 20px;
      padding-bottom: 20px;
      border-top: 1px solid var(--gray-border);
      border-bottom: 1px solid var(--gray-border);
    }

    .attribution_list {
      display: inline-block;
      list-style-type: none;
      padding: 0;
      margin: 0;
    }

    .attribution_list li {
      position: relative;
      display: flex;
      margin-bottom: 0;
      font-size: 90%;
      text-transform: capitalize;
    }

    .attribution_list li span {
      display: inline-block;
      width: 150px;
      overflow: hidden;
      white-space: nowrap;
      text-overflow: ellipsis;
      margin-right: 10px;
    }

    .attribution_list .scent {
      display: flex;
      position: relative;
      width: 40px;
      align-items: center;
    }

    .attribution_list .scent div {
      position: absolute;
      left: 0;
      background: #ccc;
      height: calc(100% - 10px);
    }

    .red { color: #c82829; }
    .orange { color: #f5871f; }
    .yellow { color: #eab700; }
    .green { color: #718c00; }
    .aqua { color: #3e999f; }
    .blue { color: #4271ae; }
    .purple { color: #8959a8; }
  </style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Interpretability as Interface Design",
  "description": "Description of the post",
  "password": "interfaces",
  "authors": [
    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Arvind Satyanarayan",
      "authorURL": "http://arvindsatya.com",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Ian Johnson",
      "authorURL": "https://github.com/enjalot",
      "affiliation": "Google Cloud",
      "affiliationURL": "http://cloud.google.com/"
    },
    {
      "author": "Ludwig Schubert",
      "authorURL": "https://schubert.io/",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Katherine Ye",
      "authorURL": "https://cs.cmu.edu/~kqy/",
      "affiliation": "CMU",
      "affiliationURL": "https://cs.cmu.edu/"
    },
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    }
  ]
  }</script>
</d-front-matter>

<d-interstitial></d-interstitial>

<d-title>
  <!-- <h1>Building Blocks<br />for Interpretability</h1> -->
  <h1>Interpretability as Interface Design</h1>
  <figure id="Teaser" class="base-grid" onclick="teaserFig.set({change_label: null});"></figure>
</d-title>

<d-article>
  <p><b><i>Author list and order not finalized.</i></b></p>

  <p>
    Designing meaningful user interfaces consists of two steps: constructing deep abstractions, and then reifying (or instantiating) them in interfaces <d-cite key="nielsen2016thought"></d-cite>.
    With a few exceptions <d-cite key="olah2015visualizing,yosinski2015understanding,carter2017using"></d-cite>, existing work on interpretability has primarily focused on the former&mdash;we have seen the development of powerful abstractions, including feature visualization <d-cite key="erhan2009visualizing,olah2017feature,simonyan2013deep,nguyen2015deep,mordvintsev2015inceptionism,nguyen2016plug"></d-cite>, attribution <d-cite key="simonyan2013deep,zeiler2014visualizing,springenberg2014striving,selvaraju2016grad,fong2017interpretable,kindermans2017patternnet,kindermans2017reliability"></d-cite>, and dimensionality reduction <d-cite key="maaten2008visualizing"></d-cite>.
    However, it appears to us that the corresponding work of reifying these abstractions has been neglected.
    This neglect has left us with impoverished interfaces (e.g., saliency maps) that leave a lot of value on the table.
    Worse, these interfaces hinder the development of our abstractions by not pushing them to their limits.
  </p>

  <p>
    When we treat interpretability as a user interface design problem, existing techniques become fundamental and composable building blocks.

    We present interfaces that show
    <i>what</i> the network detects and explain
    <i>how</i> it develops its understanding, while keeping things
    <i>human-scale</i>.

    For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification.

    Similarly, when the network looks at an image of a woman with a punching bag, we can observe the model's bias in action:
    the model recognizes a feminine body which decreases the probability of a "punching bag" classification and increase "pole dancing."
    </p>

  <!-- <p>
    In this article, we consider interpretability through the lens of user interface design: iterating between reifying abstractions and pushing them further.
    This process has allowed us to identify relatively basic operations with a lot of potential.
    For instance, feature visualization naturally combines with a network's activations to create a "semantic dictionary" that makes hidden activations meaningful.
    Moreover, we find that existing techniques can be applied in more general ways.
    Attribution, for example, can be layered over semantic dictionaries to understand how concepts evolve through the network's hidden layers.
  </p>

  <p>
    With this lens, existing techniques now become fundamental and composable building blocks for a new generation of more meaningful user interfaces for interpretability.
    We present interfaces that show <i>what</i> the network detects and explain <i>how</i> it develops its understanding, while keeping things <i>human-scale</i>.
    For example, we will see how a network looking at a labrador retriever detects floppy ears and how that influences its classification.
    Similarly, when the network looks at an image of a woman with a punching bag, we can observe the model's bias in action: the model recognizes a feminine body which decreases the probability of a "punching bag" classification and increase "pole dancing."
  </p> -->

  <p>
    From calculus to programming languages, there is a long history of new interface abstractions enabling progress in science and engineering.

    Deep learning increasingly seems like a new computing paradigm, distinct from traditional enumerative algorithms.

    If this is true, we should expect to need corresponding new abstractions.

    We believe the right abstractions will help build models that are fair, robust, and aligned.
  </p>

  <h2>Making Sense of Hidden Layers</h2>

  <p>
    Much of the recent work on interpretability is concerned with a neural network's input and output layers.
    Arguably, this focus is due to the clear meaning these layers have: in computer vision, the input layer represents values for the red, green, and blue color channels for every pixel in the input image, while the output layer consists of class labels and their associated probabilities.
  </p>

  <p>
    However, the power of neural networks lies in their hidden layers -- at every layer, the network discovers a new representation of the input.
    In computer vision, we use neural networks that run the same feature detectors at every position in the image.
    We can think of each layer's learned representation as a three-dimensional cube. Each cell in the cube is an <em>activation</em>, or the amount a neuron fires.
    The x- and y-axes correspond to positions in the image, and the z-axis is the channel (or detector) being run.
  </p>

  <figure id="ActivationCube" class="l-body"></figure>

  <figure id="SemanticDict" class="base-grid"></figure>

  <p>
    This marriage of activations and feature visualizations realizes widespread intuition for what activation vectors really are.

    It has always been suspected -- and increasingly shown -- that neurons are detecting important natural concepts.

    By turning this intuition into an interface, we change our relationship with the underlying mathematical objects.
  </p>

  <p>
    However, these concepts may not map precisely to those we are used to reasoning about, or even have good words to describe.

    Instead, they may be abstractions alien to us, or abstractions familiar to us but with deeper nuance.

    For instance, the network has multiple floppy ear detectors that seem to detect slightly different levels of droopiness and surrounding context to the ears.

    Just like translating between two languages loses nuance, shoehorning these concepts into natural language is a lossy operation.

    The neural network is learning a set of visual abstractions, and images, rather than words, are the most natural symbols to represent them.

    Were we working with audio, clips would most likely be the more natural symbols instead.

    In general, canonical examples are a more natural way to represent the foreign abstractions that neural networks learn than native human language.
  </p>

  <p>
    By bringing meaning to hidden layers, semantic dictionaries set the stage for our existing interpretability techniques to be composable building blocks.

    As we shall see, just like their underlying vectors, we can apply dimensionality reduction to them.

    In other cases, semantic dictionaries allow us to push these techniques further.

    For example, besides the one-way attribution that we currently perform with the input and output layers, semantic dictionaries allow us to attribute to-and-from specific hidden layers.

    In principle, this work could have been done without semantic dictionaries but it would have been unclear what the results meant.
  </p>

  <h2>What Does the Network See?</h2>

  <figure id="ActivationVecVis" class="base-grid" style="margin-top: 0;"></figure>

  <p>
    Applying this technique to all the activation vectors allows us to not only see what the network detects at each position, but also what the network understands of the input image as a whole.

    And, by working across layers, we can observe how the network's understanding evolves: from detecting edges in earlier layers, to more sophisticated shapes and object parts in the latter.
  </p>

  <!-- Include a blown up version of most interesting grid? -->

  <figure id="AllActivationGrids" class="act-grids full-width"></figure>

  <p>
    These visualizations, however, omit a crucial piece of information: the magnitude of the activations.
    By sizing each of the cells of the visualization by the magnitude of the activation vector, we can indicate how strongly the network detected features at that position:
  </p>

  <figure id="AllActivationGridsMagnitude" class="act-grids full-width"></figure>

  <p>
    Traditional dimensionality reduction techniques, such as principal component analysis, give us another way to understand what the network is detecting.
    For instance, to get an "overview" of the activation vectors, we can compute the first three principal components of the activations and color the image based on them.
  </p>

  <p>
    These principle components are also directions in activation space and, thus, have meaning by themselves.
    Using feature visualization, we can inspect what each of the components represents.
  </p>

  <figure class="l-page">
    <img src="images/activation-pca.png"></img>
  </figure>

  <p>
    In the end, a network's activations can be presented as semantic dictionaries with different bases.
    There are three particularly natural ones to consider: what an activation vector represents as a whole, the vector in terms of some dimensionality reduction, and finally the individual neurons themselves.
    Note, these all (approximately) describe the same object!
  </p>

  <figure id="FullVectorEq" class="l-page">
    <img src="images/vector-eq-full.png"  style="width:100%"></img>
  </figure>

  <h2>How Are Concepts Assembled?</h2>

  <p>
    Feature visualization and dimensionality reduction techniques help us answer the first of our three original questions: <em>what</em> does the network detect. However, neither technique allows us to reason about <em>how</em> the network assembled these individual pieces to arrive at later decisions, or <em>why</em> these decisions were made.
  </p>

  <p>
    Attribution is a set of techniques that answers such questions by explaining the relationships between neurons.
    There are a wide variety of approaches to attribution <d-cite key="simonyan2013deep,zeiler2014visualizing,springenberg2014striving,selvaraju2016grad,fong2017interpretable,kindermans2017patternnet"></d-cite>.
    So far, there doesn't seem to be a clear right answer.
    In fact, there's reason to think that all our present answers aren't quite right <d-cite key="kindermans2017reliability"></d-cite>.
    We think there's a lot of import research to be done on attribution methods, but for the purposes of this article the exact approach taken to attribution doesn't matter.
    We use a fairly simple method, linearly approximating the relationship, but could easily substitute in essentially any other technique.
    Future improvements to attribution will, of course, correspondingly improve the interfaces built on top of them. 
  </p>

  <h3>Spatial Attribution with Saliency Maps</h3>

  <p>
    The most common interface for attribution is called a <i>saliency map</i> -- a simple heatmap that highlights pixels of the input image that most caused the output classification.

    We see there being two weaknesses with this current approach.
  </p>

  <p>
    First, it is not clear that individual pixels should be the primary unit of attribution.

    The meaning of each pixel is extremely entangled with other pixels, is not robust to simple visual transforms (e.g., brightness, contrast, etc.), and is far-removed from high-level concepts like the output class.

    However, if we treat attribution as another user interface building block, we can instead apply them to the hidden layers of a neural network.

    In doing so, we change the questions we are asking.

    Rather than asking whether the color of a particular pixel was important for the "labrador retriever" classification, we instead ask whether the <i>high-level idea</i> detected at that position (such as "floppy ear") was important.

    This approach is similar to what Class Activation Mapping (CAM) methods<d-cite key="zhou2016learning,selvaraju2016grad"></d-cite> do, but they interpret their results back onto the input image.

    As a result, they miss the opportunity to communicate in terms of the rich behaviour of a network's hidden layers.
  </p>

  <p>
    Second, traditional saliency maps are too limited of an interface.

    They only display the attribution for a single class at a time, and do not allow you to probe into individual points more deeply.

    Moreover, as they are not explicitly dealing with hidden layers, we cannot fully explore their design space.
  </p>

  <figure id="AttributionSpatial" class="base-grid"></figure>

  <p>
    The above interface affords us a more flexible relationship with attribution.

    To start, we perform attribution from each spatial position of each hidden layer shown to all 1,000 output classes.

    In order to visualize this thousand-dimensional vector, we use dimensionality reduction to produce a multi-directional saliency map.

    Overlaying these saliency maps on our magnitude-sized activation grids provides an information scent<d-cite key="pirolli1999information"></d-cite> over attribution space.

    The activation grids allow us to anchor attribution to the visual vocabulary our semantic dictionaries first established.

    On hover, we update the legend to depict attribution to the output classes (i.e., which classes does this spatial position most contribute to?).

    Perhaps most interestingly, hovering displays additional saliency maps that mask the other hidden layers.

    In doing so, we interactively conduct layer-to-layer attribution, in a sense shining a light into the black box of these hidden layers.

    This type of layer-to-layer attribution is a prime example of how carefully considering interface design drives the generalization of our existing abstractions for interpretability.
  </p>

  <p>
    With this diagram, we have begun to think of attribution in terms of higher-level concepts.

    However, by continuing to focus on spatial positions, these concepts remain entangled.

    At a particular position, many concepts are being detected together and this interface makes it difficult to split them apart.
  </p>

  <h3>Channel Attribution</h3>

  <p>
    Saliency maps implicitly slice our cube of activations by applying attribution to the spatial positions of a hidden layer.

    This aggregates over all channels and, as a result, we cannot tell which specific detectors <i>at each position</i> most contributed to the final output classification.
  </p>

  <p>
    An alternate way to slice the cube is by channels instead of spatial locations.

    Doing so allows us to perform <i>channel attribution</i>: how much did each detector contribute to the final output?

    (This approach is similar to contemporaneous work by Kim et al.<d-cite key="kim2017tcav"></d-cite>, who do attribution to learned combination of channels.)
  </p>

  <figure class="base-grid" id="AttributionChannel"></figure>

  <p>
    This diagram is analogous to the previous one we saw: we conduct layer-to-layer attribution but this time over channels rather than spatial positions.

    Once again, we use the icons from our semantic dictionary to represent the channels that most contribute to the final output classification.

    Hovering over an individual channel displays a heatmap of its activations overlaid on the input image.

    The legend also updates to show its attribution to the output classes (i.e., what are the top classes this channel supports?).

    Clicking a channel allows us to drill into the layer-to-layer attributions, identifying the channels at lower layers that most contributed as well as the channels at higher layers that are most supported.
  </p>

  <p>
    While these diagrams focus on layer-to-layer attribution, it can still be valuable to focus on a single hidden layer.

    For example, the teaser figure allows us to evaluate hypotheses for why one class succeeded over the other.
  </p>

  <!-- <figure class="l-page">
    <img src="images/attribution-channels.png"></img>
  </figure> -->

<!--
  <h3>Neuron Attribution</h3>

  <p>Of course, these two types attributions need not live in isolation. We can bring both spatial and channel attribution together in a single coherent interface.</p>

  <figure class="l-page" id="SpatialChannelAttribution" style="border: 1px dashed #ccc; height: 200px;"></figure>
-->

  <p>
    Attribution to spatial locations and channels can reveal powerful things about a model, especially when we combine them together.

    Unfortunately, this family of approaches is burdened by two significant problems.

    On the one hand, it is very easy to end up with an overwhelming amount of information: it would take hours of human auditing to understand the long-tail of channels that slightly impact the output.

    On the other hand, both the aggregations we have explored are extremely lossy and can miss important parts of the story.

    And, while we could avoid lossy aggregation by working with individual neurons, and not aggregating at all, this explodes the first problem combinatorially.
  </p>


  <h2>Making Things Human-Scale</h2>

  <p>
    In previous sections, we've considered three ways of slicing the cube of activations: into spatial activations, channels, and individual neurons.
    Each of these has major downsides.
    If one work's with only spatial activations or channels, they miss out on very important parts of the story.
    For example it's interesting that the floppy ear detector helped us classify an image as a Labrador retriever, but it's much more interesting when that's combined with the locations that fired to do so.
    One can try to drill down to the level of neurons to tell the whole story, but the tens of thousands of neurons are simply too much information.
    Even the hundreds of channels, before being split into individual neurons, can be overwhelming to show users!
  </p>

  <p>
    If we want to make useful interfaces into neural networks, it isn't enough to make things meaningful.
    We need to make them human scale, rather than overwhelming dumps of information.
    The key to doing so is finding more meaningful ways of breaking up our activations.
    There's a lot of reason to believe that such decompositions exist.
    Often, many channels or spatial positions will work together in a highly correlated way and are most useful to think of as one unit.
    Other channels or positions will have very little activity, and can be ignore for a high-level overview.
    So, it seems like we ought to be able to find better decompositions if we had the right tools.
  </p>

  <p>
    There is an entire field of research, called matrix factorization, that studies optimal strategies for breaking up matrices. By flattening our cube into a matrix of spatial locations and channels, we can apply these techniques to get more meaningful groups of neurons. These groups will not align as naturally with the cube as the groupings we previously looked at. Instead, they will be combinations of spatial locations and channels. Moreover, these groups are constructed to explain the behavior of a network on a particular image. It would not be effective to reuse the same groupings on another image; each image requires calculating a unique set of groups.
  </p>

  <figure id="NeuronGroupsCube">
    <img src="images/cube-slices.svg"></img>
  </figure>

  <p>
    The groups that come out of this factorization will be the atoms of the interface a user works with. Unfortunately, any grouping is inherently a tradeoff between reducing things to human scale and, because any aggregation is lossy, preserving information. Matrix factorization lets us pick what our groupings are optimized for, giving us a better tradeoff than the natural groupings we saw earlier.
  </p>

  <p>
    (Recent work has explored other techniques for finding meaningful directions in activation space <d-cite key="raghu2017svcca,kim2017tcav"></d-cite>.
    While this work primarily focuses on finding "globally" meaningful directions, we instead focus on creating smaller numbers of directions to explain individual examples.)
  </p>

  <!-- NIPS Disentagling workshop. -->


  <p>
    The goals of our user interface should influence what we optimize our matrix factorization to prioritize. For example, if we want to prioritize what the network detected, we would want the factorization to fully describe the activations. If we instead wanted to prioritize what would change the network's behavior, we would want the factorization to fully describe the gradient. Finally, if we want to prioritize what caused the present behavior, we would want the factorization to fully describe the attributions. Of course, we can strike a balance between these three objectives rather than optimizing one to the exclusion of the others.
  </p>

  <p>
    In the following diagram, we've constructed groups that prioritize the activations, by factorizing the activations
        <d-footnote>Most matrix factorization algorithms and libraries are set up to minimize the mean squared error of the reconstruction of a matrix you give them, so if you just want to optimize for one thing -- in this case, the activations -- it's quite straightforward. There's ways to hack such libraries to do more general things with clever manipulations of the matrix you give it, as we'll see below. More broadly matrix factorization is just optimization, and with custom tools you can do all sorts of things.</d-footnote>
     with non-negative matrix factorization
        <d-footnote>As the name suggests, non-negative matrix factorization (NMF) constrains its factors to be positive. This is fine for the activations of a ReLU network, which must be positive as well. Our experience is that the groups we get from NMF seem more independent and semantically meaningful than those without this constraint. Because of this constraints, groups from NMF are a less efficient at representing the activations than they would be without, but our experience is that they seem more independent and semantically meaningful.</d-footnote>
     .
    Notice how the overwhelmingly large number of neurons has been reduced to a small set of groups, concisely summarizing the story of the neural network.
  </p>

  <figure class="l-page-outset">
    <img src="images/groups.png"></img>
  </figure>

  <p>
    This figure only focuses at a single layer but, as we saw earlier, it can be useful to look across multiple layers to understand how a neural network assembles together lower-level detectors into higher-level concepts.
  </p>

  <p>
    The groups we constructed before were optimized to understand a single layer independent of the others. To understand multiple layers together, we would like each layer's factorization to be "compatible"&mdash;to have the groups of earlier layers naturally compose into the groups of later layers. This is also something we can optimize the factorization for.
      <d-footnote>
        We formalize this "compatibility" in a manner described below, although we're not confident it's the best formalization and won't be surprised if it is superseded in future work.<br>
        Consider the attribution from every neuron in the layer to the set of <i>N</i> groups we want it to be compatible with.
        The basic idea is to split each entry in the activation matrix into <i>N</i> entries on the channel dimension, spreading the values proportional to the absolute value of its attribution to the corresponding group.
        Any factorization of this matrix induces a factorization of the original matrix by collapsing the duplicated entries in the column factors.
        However, the resulting factorization tries to create separate factors when the activation of the same channel has different attributions in different places.
      </d-footnote>
  </p>

  <figure id="AttributionGroups" class="l-page"></figure>

  <p>
    In this section, we recognize that the way in which we break apart the cube of activations is an important interface decision. Rather than resigning ourselves to the natural slices of the cube of activations, we construct more optimal groupings of neurons. These improved groupings are both more meaningful and more human-scale, making it less tedious for users to understand the behavior of the network.
  </p>

  <h2>The Space of Interpretability Interfaces</h2>

  <p>
    The interface ideas presented in this article combine building blocks such as feature visualization and attribution. 
    
    Composing these pieces is not an arbitrary process, but rather follows a structure based on the goals of the interface. 
    
    For example, should the interface emphasize <i>what</i> the network recognizes, prioritize <i>how</i> its understanding develops, or focus on making thing <i>human-scale</i>. 
    
    To evaluate such goals, and understand the tradeoffs, we need to be able to <i>systematically</i> consider possible alternatives.
  </p>

  <p>
    We can think of an interface as a union of individual elements. 
    
    Each element displays a specific type of <i class="green">content</i> (e.g., activations or attribution) using a particular style of <i class="blue">presentation</i> (e.g., feature visualization or traditional information visualization).

    This content lives on substrates defined by how given <i class="red">layers</i> of the network are broken apart into <i class="orange">atoms</i>, and may be <i class="green">transformed</i> by a series of operations (e.g., to filter it or project it onto another substrate).

    For example, our semantic dictionaries use <span class="blue">feature visualization</span> to display the <span class="green">activations</span> of a <span class="red">hidden layer's</span> <span class="orange">neurons</span>. 


    <!-- We find it helpful to think about these interfaces as living on a grid:  -->
    <!-- every position on this grid is an atom, content will be points and lines that connect them. -->
  </p>

  <p>
    One way to represent this way of thinking is with a formal grammar<span id="Grammar"></span>, but we find it helpful to think about the space visually.

    We can represent the network's substrate (which layers we display, and how we break them apart) as a grid, with the content and style of presentation plotted on this grid as points and connections.
  </p>

  <figure class="l-body">
    <img src="images/design_space/empty.svg"></img>
  </figure>

  <p>
    This setup gives us a framework to begin exploring the space of interpretability interfaces step by step.

    For instance, let us consider our teaser figure again. 
    
    Its goal is to help us compare two potential classifications for an input image.    
  </p>

  <figure class="l-page">
    <img src="images/design_space/teaser.svg"></img>
  </figure>

  <p>
    In this article, we have only scratched the surface of possibilities. 

    There are opportunities to further refine the individual building blocks, and lots of combinations of them yet to be explored.

    By thinking of this process as a design space, we can perform this exploration much more systematically.
  </p>

  <p>
    Moreover, as new building blocks are discovered, they will expand the space altogether.

    For example, Koh et al. suggest ways of understanding the influence of dataset examples on model behavior <d-cite key="koh2017understanding"></d-cite>.

    We can model dataset examples as another substrate in our design space, thus becoming another building block that fully composes with the others. 

    In doing so, we can now imagine interfaces that not only allow us to inspect the influence of dataset examples on the final output classification (as Koh et al. proposed) but also how examples influence the features of hidden layers, and how that influence carries through to the output. 
  </p>

  <figure class="l-page">
    <img src="images/design_space/dataset.svg"></img>
    <figcaption style="margin-top: 10px;">
      Introducing dataset examples as a new substrate in the design space. 
      On the left, interfaces that help us understand the influence of dataset examples on the output classification as presented by Koh et al<d-cite key="koh2017understanding"></d-cite>.
      On the right, interfaces that show which dataset examples caused which channels to increase the probability of the ultimate classification. 
    </figcaption>
  </figure>

  <p>
    Beyond interfaces for analyzing model behavior, if we add model <i>parameters</i> as a substrate, the design space now allows us to consider interfaces for taking action on neural networks.

    Simple such interfaces could visualize the weights learned by the model and, by making them interactive, provide an opportunity for human-in-the-loop training.
    
    More advanced interfaces could link the parameter and dataset example substrates, allowing users to modify the training set or even generate and test adversarial examples.
  </p>

  <p>
    Multiple models paragraph.
  </p>

  <!-- <p>
    The interface ideas we present in this article only scratch the surface of possibilities.

    There is a rich space of ways to combine the building blocks we have discussed.

    Composing these building blocks is not an arbitrary process -- there is a fundamental structure to how they fit together.

    Our diagrams have made a number of independent decisions: for example, which layers of the network we inspect, what atoms we break their activations into, or what content we display about these atoms. 
  </p> -->

  <h2>Conclusion &amp; Future Work</h2>

  <p>
    There is a rich design space for interacting with enumerative algorithms, and we believe an equally rich space exists for interacting with neural networks.

    These interfaces will be necessary for meaningful human oversight and critical for endorsing a model's decision-making process.
  </p>
</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>

  <h3>Author Contributions</h3>
  <p>
  </p>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
